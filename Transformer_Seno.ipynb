{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IMPLEMENTASI DAN EKSPERIMEN MODEL TRANSFORMER PADA TIGA DATASET UNTUK MENENTUKAN KINERJA TERBAIK**\n",
    "\n",
    "Nama  : Maulana Seno Aji Yudhantara  \n",
    "NRP   : 152022065  \n",
    "Kelas : IFB-454 DEEP LEARNING  \n",
    "\n",
    "## Konteks Proyek Kelompok:\n",
    "Bagian dari tugas kelompok yang menguji tiga arsitektur deep learning berbeda pada tiga dataset yang sama. Notebook ini fokus pada eksperimen dan optimasi model Transformer.\n",
    "\n",
    "## Tujuan Eksperimen:\n",
    "Notebook ini bertujuan mengimplementasikan arsitektur Transformer dan melakukan eksperimen untuk mencari konfigurasi parameter terbaik pada tiga dataset yang telah ditentukan kelompok: BBC News, Sentiment140, dan FordA (UCR Time Series).  \n",
    "Setiap dataset memiliki karakteristik berbeda (teks dan time series) sehingga diperlukan preprocessing dan penyesuaian model yang sesuai.\n",
    "\n",
    "## Dataset yang Digunakan:\n",
    "1. **BBC News**  \n",
    "   - Jenis: Teks (klasifikasi topik berita)  \n",
    "   - Deskripsi: Dataset berisi artikel berita BBC yang dikelompokkan ke dalam beberapa kategori topik.  \n",
    "\n",
    "2. **Sentiment140**  \n",
    "   - Jenis: Teks (klasifikasi sentimen tweet)  \n",
    "   - Deskripsi: Dataset tweet Twitter yang dilabeli sentimen positif, negatif, dan netral.  \n",
    "\n",
    "3. **FordA (UCR Time Series Classification)**  \n",
    "   - Jenis: Time Series (klasifikasi deret waktu)  \n",
    "   - Deskripsi: Dataset data sensor mobil Ford yang bertujuan klasifikasi kondisi berdasarkan sinyal time series.  \n",
    "\n",
    "## Rencana Implementasi:\n",
    "- Import library yang diperlukan  \n",
    "- Memuat dan eksplorasi dataset  \n",
    "- Preprocessing data sesuai kebutuhan Transformer  \n",
    "- Definisi dan pelatihan model Transformer dengan variasi parameter  \n",
    "- Evaluasi dan visualisasi hasil performa model  \n",
    "\n",
    "## Output yang Dihasilkan:\n",
    "- Source code lengkap model Transformer  \n",
    "- Hasil evaluasi performa dan parameter terbaik  \n",
    "- Penjelasan teknis dan hasil eksperimen sebagai bahan laporan kelompok  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tahap 1 – Import Semua Library** \n",
    "Melakukan import seluruh library yang dibutuhkan mulai dari data handling, preprocessing, pembuatan model, hingga evaluasi dan visualisasi. Semua import dilakukan di satu cell agar terorganisasi rapi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU:  1\n",
      "GPU Name:  NVIDIA GeForce GTX 1650\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell Code: Import Library\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Untuk pemrosesan teks\n",
    "import re\n",
    "import string\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Library untuk machine learning dan deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Untuk time series (jika perlu preprocessing khusus)\n",
    "from scipy import stats\n",
    "\n",
    "# Untuk progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tahap 2 – Load Dataset**\n",
    "Memuat ketiga dataset ke memori notebook. Setiap dataset akan dibaca sesuai format dan disiapkan untuk tahap preprocessing.\n",
    "\n",
    "- **Tahap 2.1:** Load dataset BBC News  \n",
    "- **Tahap 2.2:** Load dataset Sentiment140  \n",
    "- **Tahap 2.3:** Load dataset FordA    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tahap 2.1 – Load Dataset BBC News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label\n",
      "0  Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
      "1  Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
      "2  Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
      "3  High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
      "4  Pernod takeover talk lifts Domecq\\n\\nShares in...  business\n",
      "label\n",
      "sport            512\n",
      "business         511\n",
      "politics         418\n",
      "tech             402\n",
      "entertainment    387\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_bbcnews_dataset(data_dir):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    label_names = []\n",
    "\n",
    "    for label_name in os.listdir(data_dir):\n",
    "        label_path = os.path.join(data_dir, label_name)\n",
    "        if os.path.isdir(label_path):\n",
    "            label_names.append(label_name)\n",
    "            for filename in os.listdir(label_path):\n",
    "                file_path = os.path.join(label_path, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    text = f.read().strip()\n",
    "                    texts.append(text)\n",
    "                    labels.append(label_name)\n",
    "\n",
    "    df = pd.DataFrame({'text': texts, 'label': labels})\n",
    "    return df\n",
    "\n",
    "# Panggil fungsi\n",
    "bbcnews_dir = './Dataset/BBCNews'  \n",
    "df_bbc = load_bbcnews_dataset(bbcnews_dir)\n",
    "\n",
    "# Lihat ringkasan\n",
    "print(df_bbc.head())\n",
    "print(df_bbc['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tahap 2.2 – Load Dataset Sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     target                                               text\n",
      "0  negative             @chrishasboobs AHHH I HOPE YOUR OK!!! \n",
      "1  negative  @misstoriblack cool , i have no tweet apps  fo...\n",
      "2  negative  @TiannaChaos i know  just family drama. its la...\n",
      "3  negative  School email won't open  and I have geography ...\n",
      "4  negative                             upper airways problem \n",
      "target\n",
      "positive    50057\n",
      "negative    49943\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_sentiment140_dataset(filepath, sample_size=100000):\n",
    "    # Nama kolom sesuai dokumentasi Sentiment140\n",
    "    cols = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "    \n",
    "    # Load dataset\n",
    "    df = pd.read_csv(filepath, encoding='latin-1', names=cols, usecols=['target', 'text'])\n",
    "    \n",
    "    # Konversi label\n",
    "    df['target'] = df['target'].replace({0: 'negative', 2: 'neutral', 4: 'positive'})\n",
    "\n",
    "    # Sampling data agar tidak terlalu besar saat eksperimen\n",
    "    df_sample = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    return df_sample\n",
    "\n",
    "# Panggil fungsi\n",
    "sentiment140_path = './Dataset/Sentimen140/training.1600000.processed.noemoticon.csv'\n",
    "df_sentiment = load_sentiment140_dataset(sentiment140_path, sample_size=100000)\n",
    "\n",
    "# Lihat ringkasan\n",
    "print(df_sentiment.head())\n",
    "print(df_sentiment['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tahap 2.3 – Load Dataset FordA Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (4921, 500)\n",
      "y shape: (4921,)\n",
      "Distribusi label: [2527 2394]\n"
     ]
    }
   ],
   "source": [
    "def load_forda_dataset(train_path, test_path):\n",
    "    def read_txt(path):\n",
    "        data = np.loadtxt(path)\n",
    "        y = data[:, 0]\n",
    "        X = data[:, 1:]\n",
    "        return X, y\n",
    "\n",
    "    # Load train & test\n",
    "    X_train, y_train = read_txt(train_path)\n",
    "    X_test, y_test = read_txt(test_path)\n",
    "\n",
    "    # Gabungkan data\n",
    "    X = np.concatenate([X_train, X_test], axis=0)\n",
    "    y = np.concatenate([y_train, y_test], axis=0)\n",
    "\n",
    "    # Ubah label -1 → 0\n",
    "    y = np.where(y == -1, 0, 1)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Path\n",
    "forda_train_path = './Dataset/FordA/FordA_TRAIN.txt'\n",
    "forda_test_path = './Dataset/FordA/FordA_TEST.txt'\n",
    "\n",
    "# Load dataset\n",
    "X_forda, y_forda = load_forda_dataset(forda_train_path, forda_test_path)\n",
    "\n",
    "# Cek shape dan distribusi label\n",
    "print(\"X shape:\", X_forda.shape)\n",
    "print(\"y shape:\", y_forda.shape)\n",
    "print(\"Distribusi label:\", np.bincount(y_forda.astype(int)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tahap 3 - Preprocessing Dataset**\n",
    "Melakukan persiapan data sesuai kebutuhan tipe dataset.\n",
    "- **Tahap 3.1:** Preprocessing BBC News — pembersihan teks, tokenisasi, encoding label, dan pembagian data train-test.  \n",
    "- **Tahap 3.2:** Preprocessing Sentiment140 — pembersihan tweet (hapus URL, tanda baca, stopwords), tokenisasi, encoding label, pembagian data train-test.  \n",
    "- **Tahap 3.3:** Preprocessing FordA — normalisasi data time series, pembagian data train-test, dan reshaping data agar sesuai input Transformer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tahap 3.1 – Preprocessing BBC News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh teks: euniversity disgraceful waste a failed government scheme to offer uk university courses online has been branded a disgraceful waste by mps the euniversity was scrapped last year having attracted only students at a cost of m chief executive john beaumont was paid a bonus of despite a failure to bring\n",
      "Label numerik: 2\n",
      "Label aslinya: politics\n"
     ]
    }
   ],
   "source": [
    "# Ambil teks dan label dari dataframe\n",
    "texts_bbc = df_bbc['text'].tolist()\n",
    "labels_bbc = df_bbc['label'].tolist()\n",
    "\n",
    "# Fungsi bersihkan teks\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)               # hapus URL\n",
    "    text = re.sub(r'<.*?>', '', text)                         # hapus HTML tag\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)                      # hapus angka dan tanda baca\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()                  # hapus spasi berlebih\n",
    "    return text\n",
    "\n",
    "# Bersihkan semua teks\n",
    "texts_bbc_clean = [clean_text(text) for text in texts_bbc]\n",
    "\n",
    "# Encode label string → angka\n",
    "label_encoder_bbc = LabelEncoder()\n",
    "labels_bbc_encoded = label_encoder_bbc.fit_transform(labels_bbc)\n",
    "\n",
    "# Split data\n",
    "X_bbc_train, X_bbc_test, y_bbc_train, y_bbc_test = train_test_split(\n",
    "    texts_bbc_clean, labels_bbc_encoded, test_size=0.2, random_state=42, stratify=labels_bbc_encoded\n",
    ")\n",
    "\n",
    "# Cek hasil\n",
    "print(\"Contoh teks:\", X_bbc_train[0][:300])\n",
    "print(\"Label numerik:\", y_bbc_train[0])\n",
    "print(\"Label aslinya:\", label_encoder_bbc.inverse_transform([y_bbc_train[0]])[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tahap 3.2 – Preprocessing Sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh tweet: mommy can you bring me home a pastrami sandwich im hungry\n",
      "Label numerik: 0\n",
      "Label aslinya: negative\n"
     ]
    }
   ],
   "source": [
    "# Ambil teks dan label\n",
    "texts_sent = df_sentiment['text'].tolist()\n",
    "labels_sent = df_sentiment['target'].tolist()\n",
    "\n",
    "# Fungsi bersihkan tweet\n",
    "def clean_tweet(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)         # hapus URL\n",
    "    text = re.sub(r'@\\w+', '', text)                    # hapus mention\n",
    "    text = re.sub(r'#\\w+', '', text)                    # hapus hashtag\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)                 # hapus tanda baca\n",
    "    text = re.sub(r'\\d+', '', text)                     # hapus angka\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()            # hapus spasi berlebih\n",
    "    return text\n",
    "\n",
    "# Bersihkan semua teks tweet\n",
    "texts_sent_clean = [clean_tweet(text) for text in texts_sent]\n",
    "\n",
    "# Encode label string ke angka\n",
    "label_encoder_sent = LabelEncoder()\n",
    "labels_sent_encoded = label_encoder_sent.fit_transform(labels_sent)\n",
    "\n",
    "# Split data\n",
    "X_sent_train, X_sent_test, y_sent_train, y_sent_test = train_test_split(\n",
    "    texts_sent_clean, labels_sent_encoded, test_size=0.2, random_state=42, stratify=labels_sent_encoded\n",
    ")\n",
    "\n",
    "# Cek hasil\n",
    "print(\"Contoh tweet:\", X_sent_train[0])\n",
    "print(\"Label numerik:\", y_sent_train[0])\n",
    "print(\"Label aslinya:\", label_encoder_sent.inverse_transform([y_sent_train[0]])[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tahap 3.3 – Preprocessing FordA Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape train: (3936, 500, 1)\n",
      "Shape test: (985, 500, 1)\n",
      "Contoh label: [0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Normalisasi fitur ke rentang [0, 1]\n",
    "scaler_forda = MinMaxScaler()\n",
    "X_forda_scaled = scaler_forda.fit_transform(X_forda)\n",
    "\n",
    "# Bentuk input untuk Transformer → [samples, sequence_length, 1]\n",
    "X_forda_reshaped = X_forda_scaled[..., np.newaxis]  # tambahkan dimensi ke-3\n",
    "\n",
    "# Split data\n",
    "X_forda_train, X_forda_test, y_forda_train, y_forda_test = train_test_split(\n",
    "    X_forda_reshaped, y_forda, test_size=0.2, random_state=42, stratify=y_forda\n",
    ")\n",
    "\n",
    "# Cek hasil\n",
    "print(\"Shape train:\", X_forda_train.shape)\n",
    "print(\"Shape test:\", X_forda_test.shape)\n",
    "print(\"Contoh label:\", y_forda_train[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tahap 4 - Pembuatan Dataset dan DataLoader PyTorch**\n",
    "- Membuat class ```Dataset``` untuk masing-masing dataset\n",
    "- Membuat DataLoader untuk training dan testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tahap 4.1 – Dataset & DataLoader untuk BBCNews\n",
    "Dataset BBCNews sudah dibersihkan pada tahap sebelumnya. \n",
    "Pada tahap ini dilakukan tokenisasi menggunakan tokenizer BERT dari Huggingface (`bert-base-uncased`), \n",
    "kemudian teks dikonversi menjadi ID dan attention mask. \n",
    "\n",
    "Data dibungkus dalam class `BBCNewsDataset` dan digunakan sebagai input ke `DataLoader` PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi tokenizer (bisa pakai tokenizer umum seperti BERT tokenizer dari Huggingface)\n",
    "tokenizer_bbc = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Maksimal panjang token (bisa disesuaikan)\n",
    "MAX_LEN_BBC = 128\n",
    "\n",
    "# Dataset PyTorch\n",
    "class BBCNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenisasi\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),     # [seq_len]\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Split data BBC\n",
    "X_train_bbc, X_test_bbc, y_train_bbc, y_test_bbc = train_test_split(\n",
    "    texts_bbc_clean, labels_bbc_encoded,\n",
    "    test_size=0.2, random_state=42, stratify=labels_bbc_encoded\n",
    ")\n",
    "\n",
    "# Buat Dataset dan DataLoader\n",
    "train_dataset_bbc = BBCNewsDataset(X_train_bbc, y_train_bbc, tokenizer_bbc, MAX_LEN_BBC)\n",
    "test_dataset_bbc = BBCNewsDataset(X_test_bbc, y_test_bbc, tokenizer_bbc, MAX_LEN_BBC)\n",
    "\n",
    "train_loader_bbc = DataLoader(train_dataset_bbc, batch_size=16, shuffle=True)\n",
    "test_loader_bbc = DataLoader(test_dataset_bbc, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tahap 4.2 – Dataset & DataLoader Sentiment140 (dengan BERT)\n",
    "\n",
    "Pada tahap ini, teks dari dataset Sentiment140 dikonversi menjadi input ID dan attention mask menggunakan tokenizer BERT.  \n",
    "Label sentimen dikonversi menjadi angka dan data dibagi menjadi data training dan testing.  \n",
    "Data dibungkus ke dalam `Sentiment140Dataset` dan dimasukkan ke dalam `DataLoader`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_sentiment = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "MAX_LEN_SENTIMENT = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode label dan split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode label ke angka\n",
    "label_encoder_sentiment = LabelEncoder()\n",
    "labels_sentiment_encoded = label_encoder_sentiment.fit_transform(df_sentiment['target'])\n",
    "texts_sentiment = df_sentiment['text'].tolist()\n",
    "\n",
    "# Split data\n",
    "X_train_sent, X_test_sent, y_train_sent, y_test_sent = train_test_split(\n",
    "    texts_sentiment, labels_sentiment_encoded,\n",
    "    test_size=0.2, random_state=42, stratify=labels_sentiment_encoded\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Dataset PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment140Dataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat Dataset dan DataLoader\n",
    "train_dataset_sent = Sentiment140Dataset(X_train_sent, y_train_sent, tokenizer_sentiment, MAX_LEN_SENTIMENT)\n",
    "test_dataset_sent = Sentiment140Dataset(X_test_sent, y_test_sent, tokenizer_sentiment, MAX_LEN_SENTIMENT)\n",
    "\n",
    "train_loader_sent = DataLoader(train_dataset_sent, batch_size=32, shuffle=True)\n",
    "test_loader_sent = DataLoader(test_dataset_sent, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tahap 4.3 – Dataset & DataLoader FordA (Time Series)\n",
    "\n",
    "Pada tahap ini, data time series dari dataset FordA diubah menjadi tensor PyTorch.  \n",
    "Setiap sampel direpresentasikan dalam format `(timesteps, 1)` agar sesuai dengan input Transformer.  \n",
    "Dataset dibagi menjadi train dan test, lalu dibungkus dalam `FordADataset` dan digunakan dalam `DataLoader`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Dataset PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FordADataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)  # (samples, timesteps)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx].unsqueeze(-1)  # (timesteps,) → (timesteps, 1)\n",
    "        y = self.y[idx]\n",
    "        return {'input': x, 'label': y}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train-Test & Buat DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train-test\n",
    "X_train_ford, X_test_ford, y_train_ford, y_test_ford = train_test_split(\n",
    "    X_forda, y_forda, test_size=0.2, random_state=42, stratify=y_forda\n",
    ")\n",
    "\n",
    "# Dataset\n",
    "train_dataset_ford = FordADataset(X_train_ford, y_train_ford)\n",
    "test_dataset_ford = FordADataset(X_test_ford, y_test_ford)\n",
    "\n",
    "# DataLoader\n",
    "train_loader_ford = DataLoader(train_dataset_ford, batch_size=32, shuffle=True)\n",
    "test_loader_ford = DataLoader(test_dataset_ford, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tahap 5 – Definisi dan Inisialisasi Model Transformer**\n",
    "\n",
    "Pada tahap ini, arsitektur Transformer didefinisikan menggunakan PyTorch.\n",
    "Model menggunakan embedding layer, positional encoding, Transformer Encoder, dan pooling.\n",
    "Model ini digunakan untuk klasifikasi teks BBCNews dan Sentiment140.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tahap 5.1 Model Transformer untuk Teks (BBCNews & Sentiment140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, hidden_dim, num_layers, num_classes, dropout=0.1, max_len=512):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim, padding_idx=0)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_len, embed_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            activation='relu',\n",
    "            batch_first=True  # Agar input shape tetap (batch, seq, dim)\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # input_ids: (batch_size, seq_len)\n",
    "        x = self.embedding(input_ids)  # (batch_size, seq_len, embed_dim)\n",
    "        x = x + self.positional_encoding[:, :x.size(1), :]  # Positional encoding\n",
    "\n",
    "        # Gunakan attention mask untuk padding token (0 jadi True → skip)\n",
    "        # Transformer mask: True = masked, False = keep\n",
    "        src_key_padding_mask = ~attention_mask.bool()  # (batch_size, seq_len)\n",
    "\n",
    "        # Forward ke Transformer encoder\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        # Mean pooling (mask padding token)\n",
    "        mask = attention_mask.unsqueeze(-1).type(torch.float)  # (batch, seq, 1)\n",
    "        x = (x * mask).sum(1) / mask.sum(1)  # (batch, embed_dim)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        out = self.fc(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tahap 5.2 – Model Transformer untuk Time Series (FordA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, hidden_dim, num_layers, num_classes, dropout=0.1, max_len=500):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "\n",
    "        # Linear projection dari input ke embedding dimension\n",
    "        self.linear_proj = nn.Linear(input_dim, embed_dim)\n",
    "\n",
    "        # Positional encoding sebagai parameter learnable\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_len, embed_dim))\n",
    "\n",
    "        # Encoder Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Dropout & Output layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_dim)\n",
    "        x = self.linear_proj(x)  # (batch_size, seq_len, embed_dim)\n",
    "        x = x + self.positional_encoding[:, :x.size(1), :]  # Add positional encoding\n",
    "        x = self.transformer_encoder(x)  # (batch_size, seq_len, embed_dim)\n",
    "        x = x.mean(dim=1)  # Pooling (mean over time steps)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tahap 6 – Inisialisasi Model dan Hyperparameter Eksperimen**\n",
    "Di tahap ini, kita akan:\n",
    "\n",
    "- Menentukan hyperparameter eksperimen untuk masing-masing dataset.\n",
    "- Menginisialisasi model Transformer untuk BBCNews, Sentiment140, dan FordA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tahap 6.1 – Konfigurasi Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'bbcnews': {\n",
    "        'embed_dim': 128,\n",
    "        'num_heads': 4,\n",
    "        'hidden_dim': 256,\n",
    "        'num_layers': 2,\n",
    "        'dropout': 0.1,\n",
    "        'lr': 1e-4,\n",
    "        'num_classes': len(label_encoder_bbc.classes_)\n",
    "    },\n",
    "    'sentiment140': {\n",
    "        'embed_dim': 128,\n",
    "        'num_heads': 4,\n",
    "        'hidden_dim': 256,\n",
    "        'num_layers': 2,\n",
    "        'dropout': 0.1,\n",
    "        'lr': 1e-4,\n",
    "        'num_classes': len(label_encoder_sentiment.classes_)\n",
    "    },\n",
    "    'forda': {\n",
    "        'input_dim': 1,\n",
    "        'embed_dim': 64,\n",
    "        'num_heads': 2,\n",
    "        'hidden_dim': 128,\n",
    "        'num_layers': 2,\n",
    "        'dropout': 0.1,\n",
    "        'num_classes': 2,\n",
    "        'seq_len': 500,\n",
    "        'lr': 1e-3\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tahap 6.2 – Inisialisasi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBC News\n",
    "model_bbc = TransformerClassifier(\n",
    "    input_dim=tokenizer_bbc.vocab_size,\n",
    "    embed_dim=hyperparams['bbcnews']['embed_dim'],\n",
    "    num_heads=hyperparams['bbcnews']['num_heads'],\n",
    "    hidden_dim=hyperparams['bbcnews']['hidden_dim'],\n",
    "    num_layers=hyperparams['bbcnews']['num_layers'],\n",
    "    num_classes=hyperparams['bbcnews']['num_classes'],\n",
    "    dropout=hyperparams['bbcnews']['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Sentiment140\n",
    "model_sentiment = TransformerClassifier(\n",
    "    input_dim=tokenizer_sentiment.vocab_size,\n",
    "    embed_dim=hyperparams['sentiment140']['embed_dim'],\n",
    "    num_heads=hyperparams['sentiment140']['num_heads'],\n",
    "    hidden_dim=hyperparams['sentiment140']['hidden_dim'],\n",
    "    num_layers=hyperparams['sentiment140']['num_layers'],\n",
    "    num_classes=hyperparams['sentiment140']['num_classes'],\n",
    "    dropout=hyperparams['sentiment140']['dropout']\n",
    ").to(device)\n",
    "\n",
    "# FordA Time Series\n",
    "model_forda = TimeSeriesTransformer(\n",
    "    input_dim=hyperparams['forda']['input_dim'],\n",
    "    embed_dim=hyperparams['forda']['embed_dim'],\n",
    "    num_heads=hyperparams['forda']['num_heads'],\n",
    "    hidden_dim=hyperparams['forda']['hidden_dim'],\n",
    "    num_layers=hyperparams['forda']['num_layers'],\n",
    "    num_classes=hyperparams['forda']['num_classes'],\n",
    "    dropout=hyperparams['forda']['dropout'],\n",
    "    max_len=hyperparams['forda']['seq_len']\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persiapan optimizer dan loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function (CrossEntropy untuk klasifikasi)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (misal Adam)\n",
    "optimizer_bbc = optim.Adam(model_bbc.parameters(), lr=hyperparams['bbcnews']['lr'])\n",
    "optimizer_sentiment = optim.Adam(model_sentiment.parameters(), lr=hyperparams['sentiment140']['lr'])\n",
    "optimizer_forda = optim.Adam(model_forda.parameters(), lr=hyperparams['forda']['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fungsi train & evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = batch['input_ids'].to(device) if 'input_ids' in batch else batch['X'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch['input_ids'].to(device) if 'input_ids' in batch else batch['X'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tahap 7 – Training dan Evaluasi Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fungsi `train_one_epoch` — Melatih model selama 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, use_attention_mask=False):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Ambil input dan label\n",
    "        input_ids = batch['input_ids'].to(device) if 'input_ids' in batch else batch['inputs'].to(device)\n",
    "        labels = batch['label'].to(device) if 'label' in batch else batch['labels'].to(device)\n",
    "\n",
    "        # Optional: attention_mask jika ada dan diperlukan\n",
    "        attention_mask = None\n",
    "        if use_attention_mask and 'attention_mask' in batch:\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        # Forward pass, sesuaikan dengan ada/tidak attention_mask\n",
    "        if attention_mask is not None:\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "        else:\n",
    "            outputs = model(input_ids)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fungsi `evaluate` — Mengevaluasi model (val/test) per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device, use_attention_mask=False):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device) if 'input_ids' in batch else batch['inputs'].to(device)\n",
    "            labels = batch['label'].to(device) if 'label' in batch else batch['labels'].to(device)\n",
    "\n",
    "            attention_mask = None\n",
    "            if use_attention_mask and 'attention_mask' in batch:\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "            else:\n",
    "                outputs = model(input_ids)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop Training dan Evaluasi untuk ketiga dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Software-IF\\Anaconda3\\envs\\GPU-Check\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:505: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loader_sentiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 14\u001b[0m\n\u001b[0;32m      8\u001b[0m val_loss_bbc, val_acc_bbc \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[0;32m      9\u001b[0m     model_bbc, test_loader_bbc, criterion, device, use_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Sentiment140 (pakai attention_mask juga)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m train_loss_sent, train_acc_sent \u001b[38;5;241m=\u001b[39m train_one_epoch(\n\u001b[1;32m---> 14\u001b[0m     model_sentiment, \u001b[43mtrain_loader_sentiment\u001b[49m, criterion, optimizer_sentiment, device, use_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m val_loss_sent, val_acc_sent \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[0;32m     17\u001b[0m     model_sentiment, test_loader_sentiment, criterion, device, use_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# FordA (time series, gak ada attention_mask)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader_sentiment' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # BBCNews (misal pakai attention_mask)\n",
    "    train_loss_bbc, train_acc_bbc = train_one_epoch(\n",
    "        model_bbc, train_loader_bbc, criterion, optimizer_bbc, device, use_attention_mask=True\n",
    "    )\n",
    "    val_loss_bbc, val_acc_bbc = evaluate(\n",
    "        model_bbc, test_loader_bbc, criterion, device, use_attention_mask=True\n",
    "    )\n",
    "\n",
    "    # Sentiment140 (pakai attention_mask juga)\n",
    "    train_loss_sent, train_acc_sent = train_one_epoch(\n",
    "        model_sentiment, train_loader_sentiment, criterion, optimizer_sentiment, device, use_attention_mask=True\n",
    "    )\n",
    "    val_loss_sent, val_acc_sent = evaluate(\n",
    "        model_sentiment, test_loader_sentiment, criterion, device, use_attention_mask=True\n",
    "    )\n",
    "\n",
    "    # FordA (time series, gak ada attention_mask)\n",
    "    train_loss_forda, train_acc_forda = train_one_epoch(\n",
    "        model_forda, train_loader_forda, criterion, optimizer_forda, device, use_attention_mask=False\n",
    "    )\n",
    "    val_loss_forda, val_acc_forda = evaluate(\n",
    "        model_forda, test_loader_forda, criterion, device, use_attention_mask=False\n",
    "    )\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"BBCNews   - Train loss: {train_loss_bbc:.4f}, Train acc: {train_acc_bbc:.4f}, Val loss: {val_loss_bbc:.4f}, Val acc: {val_acc_bbc:.4f}\")\n",
    "    print(f\"Sentiment - Train loss: {train_loss_sent:.4f}, Train acc: {train_acc_sent:.4f}, Val loss: {val_loss_sent:.4f}, Val acc: {val_acc_sent:.4f}\")\n",
    "    print(f\"FordA     - Train loss: {train_loss_forda:.4f}, Train acc: {train_acc_forda:.4f}, Val loss: {val_loss_forda:.4f}, Val acc: {val_acc_forda:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tahap 7 – Kesimpulan dan Catatan**  \n",
    "Menyimpulkan temuan eksperimen, mengidentifikasi parameter terbaik, dan membuat catatan penting untuk diskusi kelompok."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
